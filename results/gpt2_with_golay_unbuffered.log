2026-01-12 23:46:15.666725: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-12 23:46:15.680873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1768261575.697939   28733 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1768261575.703451   28733 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1768261575.716832   28733 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768261575.716856   28733 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768261575.716860   28733 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768261575.716861   28733 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-12 23:46:15.720688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'evaluation.experiments.monte_carlo' found in sys.modules after import of package 'evaluation.experiments', but prior to execution of 'evaluation.experiments.monte_carlo'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
Loading model: gpt2
`torch_dtype` is deprecated! Use `dtype` instead!
Monte Carlo BER Sweep
============================================================
Model: gpt2
Cache modes: ['int4', 'int4-hamming84', 'int4-hamming84-interp', 'int12-golay']
BER levels: [0.0, 0.0001, 0.001, 0.01]
Seeds: [42, 101, 997] (3 trials/config)
Samples: 3
KL Divergence: enabled
Top-5 Accuracy: enabled
Catastrophic Rate: enabled

Generating clean baseline logits for KL divergence...
  Generated 3 clean logits

  [  0.0%] int4 @ BER=0e+00 seed=42
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
  [  2.1%] int4 @ BER=0e+00 seed=101
  [  4.2%] int4 @ BER=0e+00 seed=997
  [  6.2%] int4 @ BER=1e-04 seed=42
  [  8.3%] int4 @ BER=1e-04 seed=101
  [ 10.4%] int4 @ BER=1e-04 seed=997
  [ 12.5%] int4 @ BER=1e-03 seed=42
  [ 14.6%] int4 @ BER=1e-03 seed=101
  [ 16.7%] int4 @ BER=1e-03 seed=997
  [ 18.8%] int4 @ BER=1e-02 seed=42
  [ 20.8%] int4 @ BER=1e-02 seed=101
  [ 22.9%] int4 @ BER=1e-02 seed=997
  [ 25.0%] int4-hamming84 @ BER=0e+00 seed=42
  [ 27.1%] int4-hamming84 @ BER=0e+00 seed=101
  [ 29.2%] int4-hamming84 @ BER=0e+00 seed=997
  [ 31.2%] int4-hamming84 @ BER=1e-04 seed=42
  [ 33.3%] int4-hamming84 @ BER=1e-04 seed=101
  [ 35.4%] int4-hamming84 @ BER=1e-04 seed=997
  [ 37.5%] int4-hamming84 @ BER=1e-03 seed=42
  [ 39.6%] int4-hamming84 @ BER=1e-03 seed=101
  [ 41.7%] int4-hamming84 @ BER=1e-03 seed=997
  [ 43.8%] int4-hamming84 @ BER=1e-02 seed=42
  [ 45.8%] int4-hamming84 @ BER=1e-02 seed=101
  [ 47.9%] int4-hamming84 @ BER=1e-02 seed=997
  [ 50.0%] int4-hamming84-interp @ BER=0e+00 seed=42
  [ 52.1%] int4-hamming84-interp @ BER=0e+00 seed=101
  [ 54.2%] int4-hamming84-interp @ BER=0e+00 seed=997
  [ 56.2%] int4-hamming84-interp @ BER=1e-04 seed=42
  [ 58.3%] int4-hamming84-interp @ BER=1e-04 seed=101
  [ 60.4%] int4-hamming84-interp @ BER=1e-04 seed=997
  [ 62.5%] int4-hamming84-interp @ BER=1e-03 seed=42
  [ 64.6%] int4-hamming84-interp @ BER=1e-03 seed=101
  [ 66.7%] int4-hamming84-interp @ BER=1e-03 seed=997
  [ 68.8%] int4-hamming84-interp @ BER=1e-02 seed=42
  [ 70.8%] int4-hamming84-interp @ BER=1e-02 seed=101
  [ 72.9%] int4-hamming84-interp @ BER=1e-02 seed=997
  [ 75.0%] int12-golay @ BER=0e+00 seed=42
  [ 77.1%] int12-golay @ BER=0e+00 seed=101
  [ 79.2%] int12-golay @ BER=0e+00 seed=997
  [ 81.2%] int12-golay @ BER=1e-04 seed=42
  [ 83.3%] int12-golay @ BER=1e-04 seed=101
  [ 85.4%] int12-golay @ BER=1e-04 seed=997
  [ 87.5%] int12-golay @ BER=1e-03 seed=42
  [ 89.6%] int12-golay @ BER=1e-03 seed=101
  [ 91.7%] int12-golay @ BER=1e-03 seed=997
  [ 93.8%] int12-golay @ BER=1e-02 seed=42
  [ 95.8%] int12-golay @ BER=1e-02 seed=101
  [ 97.9%] int12-golay @ BER=1e-02 seed=997

Sweep complete!

Results saved to:
  - results/gpt2_with_golay/monte_carlo_results.json
  - results/gpt2_with_golay/results_table.txt
  - results/gpt2_with_golay/results_table.tex

PERPLEXITY (lower is better)
--------------------------------------------------------------------------------
BER   | INT4 (Unprotected) | Hamming(8,4) | H(8,4)+Interp | Golay(24,12)
------------------------------------------------------------------------
0     | 1.77               | 1.77         | 1.77          | 1.77        
1e-04 | 1.77+/-0.02        | 1.77         | 1.77          | 1.77        
1e-03 | 1.88+/-0.03        | 1.77+/-0.01  | 1.77+/-0.01   | 1.77        
1e-02 | 7.72+/-1.32        | 2.14+/-0.11  | 1.81          | 1.79+/-0.01 

KL DIVERGENCE (nats, lower is better - 0 = identical to clean)
--------------------------------------------------------------------------------
BER   | INT4 (Unprotected) | Hamming(8,4)    | H(8,4)+Interp   | Golay(24,12)   
--------------------------------------------------------------------------------
0     | 0.0166             | 0.0166          | 0.0166          | 0.0166         
1e-04 | 0.0208+/-0.0010    | 0.0166          | 0.0166          | 0.0166         
1e-03 | 0.0752+/-0.0246    | 0.0187+/-0.0035 | 0.0156+/-0.0007 | 0.0166         
1e-02 | 1.5249+/-0.0720    | 0.3127+/-0.0371 | 0.0564+/-0.0109 | 0.0234+/-0.0054

TOP-5 ACCURACY % (higher is better - 100% = target always in top 5)
--------------------------------------------------------------------------------
BER   | INT4 (Unprotected) | Hamming(8,4) | H(8,4)+Interp | Golay(24,12)
------------------------------------------------------------------------
0     | 93.9%              | 93.9%        | 93.9%         | 93.9%       
1e-04 | 94.0+/-0.1%        | 93.9%        | 93.9%         | 93.9%       
1e-03 | 93.9+/-0.4%        | 93.7+/-0.1%  | 93.7+/-0.1%   | 93.9%       
1e-02 | 84.0+/-2.7%        | 93.1+/-1.2%  | 94.1+/-0.1%   | 94.0+/-0.4% 

ERROR CORRECTION STATISTICS
--------------------------------------------------------------------------------
BER   | INT4 (Unprotected) | Hamming(8,4)       | H(8,4)+Interp      | Golay(24,12)   
--------------------------------------------------------------------------------------
0     | -                  | -                  | -                  | -              
1e-04 | -                  | 19,098 / 10        | 19,098 / 10        | 22,168         
1e-03 | -                  | 191,009 / 826      | 191,009 / 826      | 224,072        
1e-02 | -                  | 1,787,019 / 71,835 | 1,787,019 / 71,835 | 2,250,610 / 877

Note: Format is 'corrected / detected' for SECDED modes
