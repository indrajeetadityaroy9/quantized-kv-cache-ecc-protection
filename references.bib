@inproceedings{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{dubey2024llama,
  title={The Llama 3 Herd of Models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Alet, Albert and Raman, Amjad and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024},
  note={Technical Report covering Llama 3 and 3.1},
  url={https://arxiv.org/abs/2407.21783}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E and Stoica, Ion and Zhang, Hao},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  year={2023},
  url={https://arxiv.org/abs/2309.06180}
}

@article{dettmers2022llm,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022},
  note={Foundational work on outlier emergence in quantized LLMs}
}

@inproceedings{liu2024kivi,
  title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Shaochen, Zhong and Xu, Zhuoran and Braverman, Vladimir and Chen, Beidi and Hu, Xia Ben},
  booktitle={Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year={2024},
  url={https://arxiv.org/abs/2402.02750}
}

@book{macwilliams1977theory,
  title={The Theory of Error-Correcting Codes},
  author={MacWilliams, Florence J and Sloane, Neil JA},
  year={1977},
  publisher={Elsevier},
  note={The definitive reference for the algebraic construction of Golay codes and the geometry of Hamming spaces}
}

@article{huang1984algorithm,
  title={Algorithm-Based Fault Tolerance for Matrix Operations},
  author={Huang, Kuang-Hua and Abraham, Jacob A},
  journal={IEEE Transactions on Computers},
  volume={100},
  number={6},
  pages={518--528},
  year={1984},
  publisher={IEEE},
  note={Foundational work establishing the linear algebraic model for protecting matrix-vector products}
}

@article{xiao2023efficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@inproceedings{frantar2023gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{hooper2024kv,
  title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

% ============================================================================
% KV Cache Compression Methods
% ============================================================================

@inproceedings{zhang2024h2o,
  title={{H$_2$O}: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and Wang, Zhangyang and Chen, Beidi},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023},
  url={https://arxiv.org/abs/2306.14048}
}

@inproceedings{liu2023scissorhands,
  title={Scissorhands: Exploiting the Persistence of Importance Hypothesis for {LLM} {KV} Cache Compression at Test Time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  pages={52342--52364},
  year={2023},
  url={https://arxiv.org/abs/2305.17118}
}

% ============================================================================
% Attention Optimization
% ============================================================================

@article{shazeer2019fast,
  title={Fast Transformer Decoding: One Write-Head is All You Need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019},
  url={https://arxiv.org/abs/1911.02150}
}

@inproceedings{ainslie2023gqa,
  title={{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4895--4901},
  year={2023},
  url={https://arxiv.org/abs/2305.13245}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are {RNNs}: Fast Autoregressive Transformers with Linear Attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR},
  url={https://arxiv.org/abs/2006.16236}
}

@inproceedings{dao2022flashattention,
  title={{FlashAttention}: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022},
  url={https://arxiv.org/abs/2205.14135}
}

@article{dao2023flashattention2,
  title={{FlashAttention-2}: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023},
  url={https://arxiv.org/abs/2307.08691}
}

@misc{flashdecoding,
  title={Flash-Decoding for Long-Context Inference},
  author={Dao, Tri and Haziza, Daniel and Massa, Francisco and Sizov, Grigory},
  year={2023},
  howpublished={Stanford CRFM Blog},
  url={https://crfm.stanford.edu/2023/10/12/flashdecoding.html}
}

% ============================================================================
% Quantization Methods
% ============================================================================

@inproceedings{lin2024awq,
  title={{AWQ}: Activation-aware Weight Quantization for On-Device {LLM} Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  booktitle={Proceedings of Machine Learning and Systems},
  volume={6},
  year={2024},
  note={Best Paper Award},
  url={https://arxiv.org/abs/2306.00978}
}

@inproceedings{xiao2023smoothquant,
  title={{SmoothQuant}: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR},
  url={https://arxiv.org/abs/2211.10438}
}

@inproceedings{zhao2024atom,
  title={Atom: Low-Bit Quantization for Efficient and Accurate {LLM} Serving},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  booktitle={Proceedings of Machine Learning and Systems},
  volume={6},
  year={2024},
  url={https://github.com/efeslab/Atom}
}

% ============================================================================
% Error-Correcting Codes - Foundational Works
% ============================================================================

@article{hamming1950error,
  title={Error Detecting and Error Correcting Codes},
  author={Hamming, Richard W.},
  journal={The Bell System Technical Journal},
  volume={29},
  number={2},
  pages={147--160},
  year={1950},
  publisher={Nokia Bell Labs}
}

@article{golay1949notes,
  title={Notes on Digital Coding},
  author={Golay, Marcel J. E.},
  journal={Proceedings of the IRE},
  volume={37},
  pages={657},
  year={1949}
}

% ============================================================================
% Soft Errors and Radiation Effects
% ============================================================================

@article{baumann2005radiation,
  title={Radiation-Induced Soft Errors in Advanced Semiconductor Technologies},
  author={Baumann, Robert C.},
  journal={IEEE Transactions on Device and Materials Reliability},
  volume={5},
  number={3},
  pages={305--316},
  year={2005},
  publisher={IEEE}
}

% ============================================================================
% Algorithm-Based Fault Tolerance
% ============================================================================

@article{davies1997algorithm,
  title={Algorithm-Based Fault Tolerance for {LU} Factorization},
  author={Davies, J. T. and Chen, Zizhong},
  journal={Journal of Parallel and Distributed Computing},
  volume={93},
  pages={27--39},
  year={1997}
}

@inproceedings{chen2008algorithm,
  title={Algorithm-Based Fault Tolerance for Fail-Stop Failures},
  author={Chen, Zizhong},
  booktitle={IEEE International Parallel and Distributed Processing Symposium},
  pages={1--8},
  year={2008},
  organization={IEEE}
}

@article{wang1994algorithm,
  title={Algorithm-Based Fault Tolerance for {FFT} Networks},
  author={Wang, Shyue-Ling and Jou, Jing-Yang},
  journal={IEEE Transactions on Computers},
  volume={43},
  number={7},
  pages={849--854},
  year={1994},
  publisher={IEEE}
}

% ============================================================================
% Neural Network Fault Tolerance
% ============================================================================

@inproceedings{reagen2018ares,
  title={Ares: A Framework for Quantifying the Resilience of Deep Neural Networks},
  author={Reagen, Brandon and Gupta, Udit and Pentecost, Lillian and Whatmough, Paul and Lee, Sae Kyu and Mulholland, Niamh and Brooks, David and Wei, Gu-Yeon},
  booktitle={Proceedings of the 55th Annual Design Automation Conference},
  pages={17:1--17:6},
  year={2018},
  organization={ACM},
  url={https://github.com/alugupta/ares}
}

@inproceedings{li2017understanding,
  title={Understanding Error Propagation in Deep Learning Neural Network ({DNN}) Accelerators and Applications},
  author={Li, Guanpeng and Hari, Siva Kumar Sastry and Sullivan, Michael and Tsai, Timothy and Pattabiraman, Karthik and Emer, Joel and Keckler, Stephen W.},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={8:1--8:12},
  year={2017},
  organization={ACM},
  url={https://dl.acm.org/doi/10.1145/3126908.3126964}
}

@inproceedings{mahmoud2020hardnn,
  title={{HardNN}: Feature Map Vulnerability Evaluation in {CNNs}},
  author={Mahmoud, Abdulrahman and Aggarwal, Neeraj and Esmaeildoust, Alireza and Hashemi, Mehdi and Reda, Sherief and Chiu, Alex and Kim, Nam Sung and Moreau, Luc},
  booktitle={IEEE International Symposium on Workload Characterization},
  pages={171--182},
  year={2020},
  organization={IEEE}
}

@inproceedings{chen2019ranger,
  title={Ranger: Range Enforcement for {DNNs} Using Selective Clipping},
  author={Chen, Zitao and Li, Guanpeng and Pattabiraman, Karthik and DeBardeleben, Nathan},
  booktitle={IEEE/IFIP International Conference on Dependable Systems and Networks},
  pages={373--384},
  year={2019},
  organization={IEEE}
}

@inproceedings{heo2023analyzing,
  title={Analyzing the Resilience of Vision Transformers to Bit-Flip Errors},
  author={Heo, Suyeon and Kim, Sunwoo and Kim, Youngmin and Kim, Hyung-Sin},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

% ============================================================================
% Inference Optimization
% ============================================================================

@inproceedings{leviathan2023fast,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR},
  url={https://arxiv.org/abs/2211.17192}
}

@article{chen2023accelerating,
  title={Accelerating Large Language Model Decoding with Speculative Sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023},
  url={https://arxiv.org/abs/2302.01318}
}

% ============================================================================
% Systems and Compilers
% ============================================================================

@inproceedings{tillet2019triton,
  title={Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations},
  author={Tillet, Philippe and Kung, Hsiang-Tsung and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={10--19},
  year={2019},
  organization={ACM},
  url={https://dl.acm.org/doi/10.1145/3315508.3329973}
}

@inproceedings{kwon2023vllm,
  title={Efficient Memory Management for Large Language Model Serving with {PagedAttention}},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023},
  organization={ACM},
  note={Alias for kwon2023efficient},
  url={https://arxiv.org/abs/2309.06180}
}
